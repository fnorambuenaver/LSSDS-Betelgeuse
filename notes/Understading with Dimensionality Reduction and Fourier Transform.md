## Saliency Map
Using a Saliency Map to identify what it learned from a class could provide important information
[[0809 - CNN - Regularization, Saliency Map and Interpretation]]

- How could this be different from using a PCA
- There's a problem cited by *Ghorbani et al. (2019)* that introducing adversarial perturbation can lead to a way different image but provide the same class
	- So a Saliency Map may not be correlated to existing information but rather representing a coarse approximation of the learned manifold

## PCA extracting Eigenvectors
Being able to explore the high variance on image can help differentiate what are unninmportant details related to noise
- How to try this hypothesis?

## Autoencoders
Provide non linearity able to compress data in a very low dimension space
[[8. Representation Learning & Generative Learning  Using Autoencoders and GANs.pdf]]
[[0810 - Autoencoders, CNN and Generating Data]]

